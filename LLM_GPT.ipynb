{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset: 100%|██████████| 51760/51760 [00:43<00:00, 1197.08it/s]\n",
      "Epoch 1/5: 100%|██████████| 25880/25880 [2:06:34<00:00,  3.41it/s, loss=1.62]  \n",
      "Epoch 2/5: 100%|██████████| 25880/25880 [2:46:50<00:00,  2.59it/s, loss=1.5]   \n",
      "Epoch 3/5: 100%|██████████| 25880/25880 [2:48:13<00:00,  2.56it/s, loss=1.45]  \n",
      "Epoch 4/5: 100%|██████████| 25880/25880 [2:50:43<00:00,  2.53it/s, loss=1.41]  \n",
      "Epoch 5/5: 100%|██████████| 25880/25880 [2:53:05<00:00,  2.49it/s, loss=1.37]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('custom_gpt2_model\\\\tokenizer_config.json',\n",
       " 'custom_gpt2_model\\\\special_tokens_map.json',\n",
       " 'custom_gpt2_model\\\\vocab.json',\n",
       " 'custom_gpt2_model\\\\merges.txt',\n",
       " 'custom_gpt2_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open(\"custom_dataset.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "padding_token_id = tokenizer.pad_token_id  # Get the padding token ID\n",
    "max_length = 1024  # Maximum sequence length for GPT-2\n",
    "\n",
    "tokenized_data = []\n",
    "\n",
    "for item in tqdm(dataset, desc=\"Tokenizing dataset\"):\n",
    "    instruction = item[\"instruction\"]\n",
    "    output = item[\"output\"]\n",
    "    input_text = instruction + \"\\n\" + output\n",
    "    tokens = tokenizer.encode(input_text, add_special_tokens=True, max_length=max_length, truncation=True)\n",
    "    tokenized_data.append(tokens)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.data[idx]\n",
    "        return torch.tensor(tokens)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "batch_size = 2\n",
    "num_epochs = 5\n",
    "dataset = CustomDataset(tokenized_data)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda data: torch.nn.utils.rnn.pad_sequence(data, batch_first=True))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    for step, batch in progress_bar:\n",
    "        inputs = batch.to(device)\n",
    "        labels = inputs.clone()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Update the progress bar description\n",
    "        progress_bar.set_postfix(loss=total_loss / (step + 1))\n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained(\"custom_gpt2_model\")\n",
    "tokenizer.save_pretrained(\"custom_gpt2_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "Give three tips for staying healthy.\n",
      "1. Eat a balanced diet: Eating a balanced diet is essential for maintaining good health. It includes fruits, vegetables, whole grains, lean proteins, and healthy fats. Eating a diet rich in fruits, vegetables, whole grains, and lean proteins is essential for maintaining good health.\n",
      "\n",
      "2. Exercise regularly: Regular physical activity can help to improve your overall health. It is important to engage in regular physical activity, such as running, cycling, or swimming, to maintain good health.\n",
      "\n",
      "3. Get enough sleep: Lack of sleep can have negative effects on your health. Lack of sleep can lead to a range of health problems, including obesity, heart disease, and certain types of cancer. It is important to get enough rest and relaxation to help you feel better and feel more rested.!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "model_path = \"custom_gpt2_model\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "prompt = \"Give three tips for staying healthy.\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "attention_mask = torch.ones_like(input_ids)\n",
    "max_length = 200\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_ids, attention_mask=attention_mask, max_length=max_length, num_return_sequences=1,  pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hello! I'm your chatbot. Let's have a conversation. (Type 'exit' to end the conversation)\n",
      "Chatbot: Goodbye! Have a great day!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "def generate_response(prompt, model, tokenizer, max_length=200):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, attention_mask=attention_mask, max_length=max_length, num_return_sequences=1,  pad_token_id=tokenizer.eos_token_id)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "def main():\n",
    "    model_path = \"custom_gpt2_model\"\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    model.eval()\n",
    "\n",
    "    print(\"Chatbot: Hello! I'm your chatbot. Let's have a conversation. (Type 'exit' to end the conversation)\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Chatbot: Goodbye! Have a great day!\")\n",
    "            break\n",
    "\n",
    "        response = generate_response(user_input, model, tokenizer)\n",
    "        print(\"Chatbot:\", response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
